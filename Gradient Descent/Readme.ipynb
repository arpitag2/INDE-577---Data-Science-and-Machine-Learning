{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Dataset\n",
    "\n",
    "Source: UC Irvine Libraries\n",
    "\n",
    "The dataset consists of red wine variant of Portuges \"Vinho Verde\" wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
    "\n",
    "For this algorithm, I use the dataset for regression.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "For more information, read [Cortez et al., 2009]. \n",
    "Input variables (based on physicochemical tests): \n",
    "1 - fixed acidity \n",
    "2 - volatile acidity \n",
    "3 - citric acid \n",
    "4 - residual sugar \n",
    "5 - chlorides \n",
    "6 - free sulfur dioxide \n",
    "7 - total sulfur dioxide \n",
    "8 - density \n",
    "9 - pH \n",
    "10 - sulphates \n",
    "11 - alcohol \n",
    "Output variable (based on sensory data): \n",
    "12 - quality (score between 0 and 10)\n",
    "\n",
    "\n",
    "I have used Alcohol (attribute 11) and pH (attribute 9) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorith that minimizes an objective convex function using iteration.\n",
    "\n",
    "![Alt text](1*SHb4QPkl02FzT8QtHFRVlQ.webp)\n",
    "\n",
    "In this file, we have used gradient descent for a linear model.\n",
    "\n",
    "In a linear model\n",
    "\n",
    "Error = Y(predicted) - Y(actual)\n",
    "\n",
    "The minimisation of error is done through gradient descent. In this, the model walks some steps (called learning rate) to reach the minimum error.\n",
    "\n",
    "Steps:\n",
    "1. Take random intital values\n",
    "\n",
    "2. Update values using the following formula:\n",
    "\n",
    "![Alt text](Gradient%20Descent%20Formula.webp)\n",
    "\n",
    "3. Repeat until the slop becomes zero.\n",
    "\n",
    "It has to be noted that the learning rate needs to be chosen wisely. Too high learning rate will result in model converging as the pointer will kepp on shooting, while too low would mean it would take too much time for the model to achieve minimum.\n",
    "\n",
    "![Alt text](Learning%20Rate.webp)\n",
    "\n",
    "\n",
    "Source: https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
