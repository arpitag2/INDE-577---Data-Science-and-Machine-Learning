{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Dataset\n",
    "\n",
    "Source: UC Irvine Libraries\n",
    "\n",
    "The dataset consists of red wine variant of Portuges \"Vinho Verde\" wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
    "\n",
    "For this analysis, I use a subset of the data with wine quality either 4 or 7. The reason being that the dataset is closest to linearly separable for this subset.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "For more information, read [Cortez et al., 2009]. \n",
    "Input variables (based on physicochemical tests): \n",
    "1 - fixed acidity \n",
    "2 - volatile acidity \n",
    "3 - citric acid \n",
    "4 - residual sugar \n",
    "5 - chlorides \n",
    "6 - free sulfur dioxide \n",
    "7 - total sulfur dioxide \n",
    "8 - density \n",
    "9 - pH \n",
    "10 - sulphates \n",
    "11 - alcohol \n",
    "Output variable (based on sensory data): \n",
    "12 - quality (score between 0 and 10)\n",
    "\n",
    "\n",
    "I have used Alcohol (attribute 11) and pH (attribute 9) for these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "In ensemble learning, multiple models are combined to get better results. These models are often \"weak learners\" which when correctly combined give accurate models.\n",
    "\n",
    "\n",
    "## Bagging\n",
    "\n",
    "- Weak learnres are homogenous\n",
    "- Each model learns independently and in parallel\n",
    "- Then models are combined through a deterministic average process\n",
    "- Focuses on less variance\n",
    "\n",
    "![Alt text](Bagging.webp)\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Learning tress (base models for ensemble methods) are combined to form 'Forests.' In Random Forests, Deep Trees (low bias high variance) are combined to produce an output with lower variance. In RF, bootstrap samples are also built from features, thus all trees do not have the same exact information.\n",
    "\n",
    "![Alt text](Random%20Forests.webp)\n",
    "\n",
    "## Boosting\n",
    "\n",
    "- Weak learnres are homogenous\n",
    "- Learning is done in sequentially in an adaptive way \n",
    "- Then models are combined through a deterministic way\n",
    "- Focuses on less bias\n",
    "\n",
    "![Alt text](Boosting.webp)\n",
    "\n",
    "\n",
    "\n",
    "Source: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
